{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ronakpatel/Concordia_Assignments/Sem 4 Fall 2023/Applied AI Project/AppliedAI/Part_2/experiments.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#W0sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m classes:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#W0sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     images \u001b[39m=\u001b[39m glob\u001b[39m.\u001b[39mglob(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_dir, c, \u001b[39m'\u001b[39m\u001b[39m*.jpg\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#W0sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     train, test \u001b[39m=\u001b[39m train_test_split(images, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#W0sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     test, val \u001b[39m=\u001b[39m train_test_split(test, test_size\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#W0sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m train:\n",
      "File \u001b[0;32m~/.virtualenvs/dl4cv/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2422\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2419\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[1;32m   2421\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m-> 2422\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2423\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[1;32m   2424\u001b[0m )\n\u001b[1;32m   2426\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m   2427\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.virtualenvs/dl4cv/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2098\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2095\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[1;32m   2097\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2098\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2099\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2100\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2101\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2102\u001b[0m     )\n\u001b[1;32m   2104\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "#Design a code that goes into Data directory in which I have four different classes of images. My Script should visit each class directory and divide each specifi class images randomly into train, test and validation sets using 70%, 15%, 15% ratio respectively. DO not modify the original data directory. Create a new directory for each class in which you have train, test and validation directories. Each of these directories should have the images of that specific class. Selection of images should be random for each classes.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Path to the data directory\n",
    "data_dir = 'AppliedAI/Data'\n",
    "\n",
    "\n",
    "# Path to the new directory\n",
    "new_dir = 'AppliedAI/Data'\n",
    "\n",
    "\n",
    "# List of classes\n",
    "classes = ['angry', 'bored', 'focused', 'neutral']\n",
    "\n",
    "# Creating new directories\n",
    "os.mkdir(new_dir)\n",
    "for c in classes:\n",
    "    os.mkdir(os.path.join(new_dir, c))\n",
    "    os.mkdir(os.path.join(new_dir, c, 'train'))\n",
    "    os.mkdir(os.path.join(new_dir, c, 'test'))\n",
    "    os.mkdir(os.path.join(new_dir, c, 'val'))\n",
    "    \n",
    "# Copying images to new directories\n",
    "for c in classes:\n",
    "    images = glob.glob(os.path.join(data_dir, c, '*.jpg'))\n",
    "    train, test = train_test_split(images, test_size=0.3)\n",
    "    test, val = train_test_split(test, test_size=0.5)\n",
    "    for img in train:\n",
    "        shutil.copy(img, os.path.join(new_dir, c, 'train'))\n",
    "    for img in test:\n",
    "        shutil.copy(img, os.path.join(new_dir, c, 'test'))\n",
    "    for img in val:\n",
    "        shutil.copy(img, os.path.join(new_dir, c, 'val'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ronakpatel/Concordia_Assignments/Sem 4 Fall 2023/Applied AI Project/AppliedAI/Part_2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get current working directory TO A SPECIFIC PATH\n",
    "os.chdir('/Users/ronakpatel/Concordia_Assignments/Sem 4 Fall 2023/Applied AI Project/AppliedAI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ronakpatel/Concordia_Assignments/Sem 4 Fall 2023/Applied AI Project'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split and copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Set the path to your Data directory\n",
    "data_dir = 'AppliedAI/Data'\n",
    "\n",
    "# Define the ratio for train, test, and validation sets\n",
    "train_ratio = 0.7\n",
    "test_ratio = 0.15\n",
    "validation_ratio = 0.15\n",
    "\n",
    "# Define the path where you want to store the new dataset\n",
    "output_dir = 'AppliedAI/Part_2/New_Data'\n",
    "\n",
    "# Get a list of all subdirectories (classes) in the Data directory\n",
    "classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "\n",
    "# Create output directories for each class (train, test, validation)\n",
    "for class_name in classes:\n",
    "    class_output_dir = os.path.join(output_dir, class_name)\n",
    "    os.makedirs(os.path.join(class_output_dir, 'train'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(class_output_dir, 'test'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(class_output_dir, 'validation'), exist_ok=True)\n",
    "\n",
    "    # List all image files in the current class directory\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    images = [f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
    "\n",
    "    # Shuffle the list of images randomly\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Split images into train, test, and validation sets\n",
    "    total_images = len(images)\n",
    "    train_split = int(total_images * train_ratio)\n",
    "    test_split = int(total_images * test_ratio)\n",
    "\n",
    "    train_images = images[:train_split]\n",
    "    test_images = images[train_split:train_split + test_split]\n",
    "    validation_images = images[train_split + test_split:]\n",
    "\n",
    "    # Copy images to their respective directories\n",
    "    for image in train_images:\n",
    "        shutil.copy(os.path.join(class_dir, image), os.path.join(class_output_dir, 'train', image))\n",
    "    for image in test_images:\n",
    "        shutil.copy(os.path.join(class_dir, image), os.path.join(class_output_dir, 'test', image))\n",
    "    for image in validation_images:\n",
    "        shutil.copy(os.path.join(class_dir, image), os.path.join(class_output_dir, 'validation', image))\n",
    "\n",
    "print(\"Data split and copied successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split and copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the path to your Data directory\n",
    "data_dir = 'AppliedAI/Data'\n",
    "\n",
    "# Define the ratio for train, test, and validation sets\n",
    "train_ratio = 0.7\n",
    "test_ratio = 0.15\n",
    "validation_ratio = 0.15\n",
    "\n",
    "# Define the path where you want to store the new dataset\n",
    "output_dir = 'AppliedAI/Part_2/New_Data'\n",
    "\n",
    "# Get a list of all subdirectories (classes) in the Data directory\n",
    "classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "\n",
    "# Create output directories for each class (train, test, validation)\n",
    "for class_name in classes:\n",
    "    class_output_dir = os.path.join(output_dir, class_name)\n",
    "    os.makedirs(os.path.join(class_output_dir, 'train'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(class_output_dir, 'test'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(class_output_dir, 'validation'), exist_ok=True)\n",
    "\n",
    "    # List all image files in the current class directory\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    images = [f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
    "\n",
    "    # Shuffle the list of images randomly\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Split images into train, test, and validation sets using scikit-learn\n",
    "    train_images, remaining_images = train_test_split(images, test_size=(test_ratio + validation_ratio), random_state=42)\n",
    "    test_images, validation_images = train_test_split(remaining_images, test_size=(validation_ratio / (test_ratio + validation_ratio)), random_state=42)\n",
    "\n",
    "    # Copy images to their respective directories\n",
    "    for image in train_images:\n",
    "        shutil.copy(os.path.join(class_dir, image), os.path.join(class_output_dir, 'train', image))\n",
    "    for image in test_images:\n",
    "        shutil.copy(os.path.join(class_dir, image), os.path.join(class_output_dir, 'test', image))\n",
    "    for image in validation_images:\n",
    "        shutil.copy(os.path.join(class_dir, image), os.path.join(class_output_dir, 'validation', image))\n",
    "\n",
    "print(\"Data split and copied successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in /Users/ronakpatel/Concordia_Assignments/Sem 4 Fall 2023/Applied AI Project/AppliedAI/Part_2/New_Data/angry/train.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ronakpatel/Concordia_Assignments/Sem 4 Fall 2023/Applied AI Project/AppliedAI/Part_2/experiments.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),  \u001b[39m# Convert images to tensors\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     transforms\u001b[39m.\u001b[39mNormalize((\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m), (\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m))  \u001b[39m# Normalize pixel values\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m ])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Define the dataset and dataloaders for train, validation, and test\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mImageFolder(\u001b[39m'\u001b[39;49m\u001b[39m/Users/ronakpatel/Concordia_Assignments/Sem 4 Fall 2023/Applied AI Project/AppliedAI/Part_2/New_Data/angry/train\u001b[39;49m\u001b[39m'\u001b[39;49m, transform\u001b[39m=\u001b[39;49mtransform)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m validation_dataset \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39mImageFolder(\u001b[39m'\u001b[39m\u001b[39m/Users/ronakpatel/Concordia_Assignments/Sem 4 Fall 2023/Applied AI Project/AppliedAI/Part_2/New_Data/angry/validation\u001b[39m\u001b[39m'\u001b[39m, transform\u001b[39m=\u001b[39mtransform)\n",
      "File \u001b[0;32m~/.virtualenvs/dl4cv/lib/python3.9/site-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    310\u001b[0m         root,\n\u001b[1;32m    311\u001b[0m         loader,\n\u001b[1;32m    312\u001b[0m         IMG_EXTENSIONS \u001b[39mif\u001b[39;49;00m is_valid_file \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    313\u001b[0m         transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[1;32m    314\u001b[0m         target_transform\u001b[39m=\u001b[39;49mtarget_transform,\n\u001b[1;32m    315\u001b[0m         is_valid_file\u001b[39m=\u001b[39;49mis_valid_file,\n\u001b[1;32m    316\u001b[0m     )\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples\n",
      "File \u001b[0;32m~/.virtualenvs/dl4cv/lib/python3.9/site-packages/torchvision/datasets/folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transform\u001b[39m=\u001b[39mtransform, target_transform\u001b[39m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 144\u001b[0m     classes, class_to_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_classes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n\u001b[1;32m    145\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader \u001b[39m=\u001b[39m loader\n",
      "File \u001b[0;32m~/.virtualenvs/dl4cv/lib/python3.9/site-packages/torchvision/datasets/folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(\u001b[39mself\u001b[39m, directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[1;32m    192\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[39m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mreturn\u001b[39;00m find_classes(directory)\n",
      "File \u001b[0;32m~/.virtualenvs/dl4cv/lib/python3.9/site-packages/torchvision/datasets/folder.py:42\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     40\u001b[0m classes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(entry\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mscandir(directory) \u001b[39mif\u001b[39;00m entry\u001b[39m.\u001b[39mis_dir())\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find any class folder in \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m class_to_idx \u001b[39m=\u001b[39m {cls_name: i \u001b[39mfor\u001b[39;00m i, cls_name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(classes)}\n\u001b[1;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m classes, class_to_idx\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in /Users/ronakpatel/Concordia_Assignments/Sem 4 Fall 2023/Applied AI Project/AppliedAI/Part_2/New_Data/angry/train."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set the device to use (CPU or GPU if available)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define data transformations for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize pixel values\n",
    "])\n",
    "\n",
    "# Define the dataset and dataloaders for train, validation, and test\n",
    "train_dataset = datasets.ImageFolder('/Users/ronakpatel/Concordia_Assignments/Sem 4 Fall 2023/Applied AI Project/AppliedAI/Part_2/New_Data/angry/train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "validation_dataset = datasets.ImageFolder('/Users/ronakpatel/Concordia_Assignments/Sem 4 Fall 2023/Applied AI Project/AppliedAI/Part_2/New_Data/angry/validation', transform=transform)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=64)\n",
    "\n",
    "test_dataset = datasets.ImageFolder('/Users/ronakpatel/Concordia_Assignments/Sem 4 Fall 2023/Applied AI Project/AppliedAI/Part_2/New_Data/angry/test', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Define a simple CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 32 * 32, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Create the model and move it to the device\n",
    "model = CNNModel(num_classes=4).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy}%\")\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.1.0-cp39-none-macosx_10_9_x86_64.whl.metadata (24 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.16.0-cp39-cp39-macosx_10_13_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typing-extensions (from torch)\n",
      "  Downloading typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /Users/ronakpatel/.virtualenvs/dl4cv/lib/python3.9/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ronakpatel/.virtualenvs/dl4cv/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/ronakpatel/.virtualenvs/dl4cv/lib/python3.9/site-packages (from torch) (2023.1.0)\n",
      "Requirement already satisfied: numpy in /Users/ronakpatel/.virtualenvs/dl4cv/lib/python3.9/site-packages (from torchvision) (1.26.1)\n",
      "Requirement already satisfied: requests in /Users/ronakpatel/.virtualenvs/dl4cv/lib/python3.9/site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ronakpatel/.virtualenvs/dl4cv/lib/python3.9/site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ronakpatel/.virtualenvs/dl4cv/lib/python3.9/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ronakpatel/.virtualenvs/dl4cv/lib/python3.9/site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ronakpatel/.virtualenvs/dl4cv/lib/python3.9/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ronakpatel/.virtualenvs/dl4cv/lib/python3.9/site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ronakpatel/.virtualenvs/dl4cv/lib/python3.9/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m686.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.0-cp39-none-macosx_10_9_x86_64.whl (147.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.0/147.0 MB\u001b[0m \u001b[31m805.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.16.0-cp39-cp39-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m824.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, filelock, torch, torchvision\n",
      "Successfully installed filelock-3.13.1 mpmath-1.3.0 sympy-1.12 torch-2.1.0 torchvision-0.16.0 typing-extensions-4.8.0\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#install pytorch using pip\n",
    "! pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(2,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ronakpatel/.virtualenvs/dl4cv/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split and directory structure preserved successfully.\n"
     ]
    }
   ],
   "source": [
    "#Final Script but refactoring needed \n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Directory containing your dataset\n",
    "dataset_dir = \"/Users/ronakpatel/Concordia_Assignments/Sem 4 Fall 2023/Applied AI Project/AppliedAI/Data\"\n",
    "\n",
    "# Directory to store the split dataset\n",
    "split_dataset_dir = \"/Users/ronakpatel/Concordia_Assignments/Sem 4 Fall 2023/Applied AI Project/AppliedAI/Part_2/dataset_inshape\"\n",
    "\n",
    "# Create the split_dataset_dir if it doesn't exist\n",
    "if not os.path.exists(split_dataset_dir):\n",
    "    os.makedirs(split_dataset_dir)\n",
    "\n",
    "# List of class labels (folder names)\n",
    "class_labels = os.listdir(dataset_dir)\n",
    "\n",
    "# Set the test size and random state\n",
    "test_size = 0.15\n",
    "random_state = int(time.time() % 1000)\n",
    "\n",
    "# Loop through each class label directory\n",
    "for label in class_labels:\n",
    "    label_dir = os.path.join(dataset_dir, label)\n",
    "    train_dir = os.path.join(split_dataset_dir, \"train\", label)\n",
    "    test_dir = os.path.join(split_dataset_dir, \"test\", label)\n",
    "\n",
    "    # Create the train and test directories for each class label\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Get a list of all files in the current class label directory\n",
    "    if os.path.isdir(label_dir):\n",
    "        files = os.listdir(label_dir)\n",
    "\n",
    "        # Split the files into train and test sets\n",
    "        train_files, test_files = train_test_split(files, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        # Move the files to their respective directories\n",
    "        for file in train_files:\n",
    "            src = os.path.join(label_dir, file)\n",
    "            dst = os.path.join(train_dir, file)\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "        for file in test_files:\n",
    "            src = os.path.join(label_dir, file)\n",
    "            dst = os.path.join(test_dir, file)\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "print(\"Data split and directory structure preserved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dimensions (type,img_dimen,kernal_size,stride_size,padding_size,number_of_filters):\n",
    "    ans=[] # \n",
    "    \n",
    "    if(type == \"conv\"):\n",
    "        new_img_dim = (img_dimen - kernal_size + 2*padding_size)/stride_size + 1\n",
    "        ans.append(new_img_dim)\n",
    "        ans.append(number_of_filters)\n",
    "        \n",
    "    elif(type == \"pool\"):\n",
    "        new_img_dim = (img_dimen - kernal_size)/stride_size + 1\n",
    "        ans.append(new_img_dim)\n",
    "        ans.append(number_of_filters)\n",
    "        \n",
    "    return ans\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44.0, 10]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "calculate_dimensions(\"conv\",48,5,2,0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47.0, 10]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_dimensions(\"pool\",48,2,1,0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "conv1 = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 48, 48])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1(torch.randn(1,4,48,48)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "res2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 12, 12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2(torch.randn(1,128,12,12)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 2, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn7(torch.randn(1,256,6,6)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=16, kernel_size=5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 22, 22])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn1(torch.randn(1,4,48,48)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 11, 11])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn2(torch.randn(1,16,24,24)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=4, out_channels=16, kernel_size=5),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2)\n",
    ")\n",
    "\n",
    "cnn2 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2)\n",
    ")\n",
    "\n",
    "cnn3 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 1)\n",
    ")\n",
    "\n",
    "cnn4 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 1)\n",
    ")\n",
    "\n",
    "cnn5 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2)\n",
    ")\n",
    "\n",
    "cnn6 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2)\n",
    ")\n",
    "\n",
    "cnn7 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 9, 9])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2(torch.randn(1,128,9,9)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_functions as helperfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def calcDimensons(type,width, height,  kernel_size, stride=1,padding=0):\n",
    "    \"\"\"\n",
    "    Calculate the output dimensions of a convolutional layer\n",
    "    \"\"\"\n",
    "    if type == \"conv\":\n",
    "        out_width = ((width + (2 * padding) - (kernel_size)) / stride) + 1\n",
    "        out_height = ((height + (2 * padding) - (kernel_size))/ stride) + 1\n",
    "    elif type == \"pool\":\n",
    "        out_width = ((width - (kernel_size)) / stride) + 1\n",
    "        out_height = ((height - (kernel_size)) / stride) + 1\n",
    "    \n",
    "    \n",
    "    return int(out_width), int(out_height)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 22)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcDimensons(\"pool\",44,44,0,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stroring network architecture in a list first is the type of layer, second is the kernel size, third is the stride, fourth is the padding\n",
    "architecture_specification = [[\"conv\",48,48,3,1,0],[\"conv\",3,1],[\"pool\",2,2],[\"conv\",3,1],[\"conv\",3,1],[\"pool\",2,2]]\n",
    "#given architecture_specification design a function that will return the final output dimensions of the last layer\n",
    "\n",
    "def getFinalOutputDimen(architecture_specification,input_width,input_height):\n",
    "    width = input_width\n",
    "    height = input_height\n",
    "    \n",
    "    for i in architecture_specification:\n",
    "        if i[0] == \"conv\":\n",
    "            width,height = calcDimensons(i[0],width,height,i[3],i[4],i[5])\n",
    "            print(width)\n",
    "        else:\n",
    "            width,height = calcDimensons(i[0],width,height,i[1],i[2])\n",
    "            print(\"inner else\")\n",
    "            print(width)\n",
    "    return width,height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ronakpatel/Concordia_Assignments/Sem 4 Fall 2023/Applied AI Project/AppliedAI/Part_2/experiments.ipynb Cell 34\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#given \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m getFinalOutputDimen(architecture_specification,\u001b[39m48\u001b[39;49m,\u001b[39m48\u001b[39;49m)\n",
      "\u001b[1;32m/Users/ronakpatel/Concordia_Assignments/Sem 4 Fall 2023/Applied AI Project/AppliedAI/Part_2/experiments.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m architecture_specification:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#X51sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mif\u001b[39;00m i[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mconv\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#X51sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         width,height \u001b[39m=\u001b[39m calcDimensons(i[\u001b[39m0\u001b[39m],width,height,i[\u001b[39m3\u001b[39;49m],i[\u001b[39m4\u001b[39m],i[\u001b[39m5\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#X51sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39mprint\u001b[39m(width)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronakpatel/Concordia_Assignments/Sem%204%20Fall%202023/Applied%20AI%20Project/AppliedAI/Part_2/experiments.ipynb#X51sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#given \n",
    "\n",
    "getFinalOutputDimen(architecture_specification,48,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=16, kernel_size=5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 22, 22])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn1(torch.randn(1,4,48,48)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after the last convolution/pool layer: (1, 64, 24, 24)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def calculate_output_shape(input_shape, layer_configs, num_input_channels):\n",
    "    # Create a sequential model to simulate the layers\n",
    "    model = nn.Sequential()\n",
    "    in_channels = num_input_channels  # Updated number of input channels\n",
    "\n",
    "    # Iterate through the layer configurations\n",
    "    for layer_config in layer_configs:\n",
    "        layer_type, *params = layer_config\n",
    "\n",
    "        if layer_type == 'conv':\n",
    "            out_channels, kernel_size, stride, padding = params\n",
    "            layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n",
    "            in_channels = out_channels\n",
    "        elif layer_type == 'pool':\n",
    "            kernel_size, stride = params\n",
    "            layer = nn.MaxPool2d(kernel_size, stride=stride)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown layer type: {layer_type}\")\n",
    "\n",
    "        model.add_module(f'layer_{len(model) + 1}', layer)\n",
    "\n",
    "    # Generate a random input tensor to get the output shape\n",
    "    input_tensor = torch.randn(1, num_input_channels, *input_shape[1:])\n",
    "    output_tensor = model(input_tensor)\n",
    "\n",
    "    return tuple(output_tensor.size())\n",
    "\n",
    "# Example usage with 32 input channels\n",
    "input_shape = (1, 48, 48)  # (channels, height, width)\n",
    "layer_configs = [\n",
    "    ('conv', 64, 3, 1, 1),\n",
    "    ('conv', 64, 3, 1, 1),\n",
    "    ('pool', 2, 2),\n",
    "    # ('conv', 128, 3, 1, 1),\n",
    "    # ('conv', 128, 3, 1, 1),\n",
    "    # ('pool', 2, 2),\n",
    "    # ('conv', 256, 3, 1, 1),\n",
    "    # ('conv', 256, 3, 1, 1),\n",
    "    # ('conv', 256, 3, 1, 1),\n",
    "    # ('pool', 2, 2),\n",
    "    # ('conv', 512, 3, 1, 1),\n",
    "    # ('conv', 512, 3, 1, 1),\n",
    "    # ('conv', 512, 3, 1, 1),\n",
    "    # ('pool', 2, 2),\n",
    "    # ('conv', 512, 3, 1, 1),\n",
    "    # ('conv', 512, 3, 1, 1),\n",
    "    # ('conv', 512, 3, 1, 1),\n",
    "    # ('pool', 2, 2)\n",
    "    \n",
    "    # ('conv', 128, 5, 1, 0),\n",
    "    # ('pool', 2, 2)\n",
    "    # ('conv', 128, 3, 1, 0),\n",
    "    # ('pool', 2, 2),\n",
    "    # ('conv', 256, 3, 1, 0),\n",
    "    # ('pool', 2, 2),\n",
    "    # ('conv', 256, 3, 1, 0),\n",
    "    # ('pool', 2, 2),\n",
    "]\n",
    "\n",
    "output_shape = calculate_output_shape(input_shape, layer_configs, num_input_channels=1)\n",
    "print(\"Output shape after the last convolution/pool layer:\", output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../best.yml\", 'a') as f:\n",
    "        f.write(\"txt\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
